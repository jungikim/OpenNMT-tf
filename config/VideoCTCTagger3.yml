params:
  maximum_iterations: 250
  beam_width: 4
  average_loss_in_time: true

  optimizer: LazyAdamOptimizer
  optimizer_params:
    beta1: 0.9
    beta2: 0.998

  learning_rate: 0.01
  clip_gradients: 3.0

  decay_type: noam_decay_v2
  decay_params:
    model_dim: 256
    warmup_steps: 2000
  start_decay_steps: 0

train:
  batch_size: 4
  bucket_width: 1
  save_checkpoints_steps: 5000
  keep_checkpoint_max: 0
  save_summary_steps: 50
  train_steps: 500000
  maximum_features_length: 500
  maximum_labels_length: 10

  # Consider setting this to -1 to match the number of training examples.
  sample_buffer_size: -1

eval:
  batch_size: 4
  eval_delay: 3600  # Every 1 hour

infer:
  batch_size: 1
