params:
  maximum_iterations: 250
  beam_width: 4
  average_loss_in_time: true

  optimizer: LazyAdamOptimizer
  optimizer_params:
    beta1: 0.9
    beta2: 0.998

  learning_rate: 2.0
  clip_gradients: 5.0

  decay_type: noam_decay_v2
  decay_params:
    model_dim: 512
    warmup_steps: 2000
  start_decay_steps: 0

  average_loss_in_time: true
  label_smoothing: 0.1

  beam_width: 4
  length_penalty: 0.6

train:
  batch_size: 3072 
  effective_batch_size: 25000 
  batch_type: tokens

  save_checkpoints_steps: 5000
  keep_checkpoint_max: 0
  save_summary_steps: 50
  train_steps: 5000000
  maximum_features_length: 1000
  maximum_labels_length: 30


  # Consider setting this to -1 to match the number of training examples.
  sample_buffer_size: -1

eval:
  batch_size: 16
  eval_delay: 3600  # Every 1 hour

infer:
  batch_size: 16
