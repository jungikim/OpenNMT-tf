params:
  optimizer: GradientDescentOptimizer
  learning_rate: 1.0
  param_init: 0.1
  clip_gradients: 5.0
  decay_type: exponential_decay
  decay_params:
    decay_rate: 0.7
    decay_steps: 100000
  start_decay_steps: 500000
  beam_width: 5
  maximum_iterations: 250
  freeze_variables:
    - seqtagger/encoder/encoder_0/Kaffe/conv1/weights:0
    - seqtagger/encoder/encoder_0/Kaffe/conv1/biases:0
    - seqtagger/encoder/encoder_0/Kaffe/conv2/weights:0
    - seqtagger/encoder/encoder_0/Kaffe/conv2/biases:0
    - seqtagger/encoder/encoder_0/Kaffe/conv3/weights:0
    - seqtagger/encoder/encoder_0/Kaffe/conv3/biases:0
    - seqtagger/encoder/encoder_0/Kaffe/conv4/weights:0
    - seqtagger/encoder/encoder_0/Kaffe/conv4/biases:0
    - seqtagger/encoder/encoder_0/Kaffe/conv5/weights:0
    - seqtagger/encoder/encoder_0/Kaffe/conv5/biases:0
    - seqtagger/encoder/encoder_0/Kaffe/fc6/weights:0
    - seqtagger/encoder/encoder_0/Kaffe/fc6/biases:0
    - seqtagger/encoder/encoder_0/Kaffe/fc7/weights:0
    - seqtagger/encoder/encoder_0/Kaffe/fc7/biases:0
    - seqtagger/encoder/encoder_0/Kaffe/fc8/weights:0
    - seqtagger/encoder/encoder_0/Kaffe/fc8/biases:0

train:
  batch_size: 1
  bucket_width: 1
  save_checkpoints_steps: 5000
  save_summary_steps: 50
  train_steps: 1000000
  maximum_features_length: 300
  maximum_labels_length: 10

  # Consider setting this to -1 to match the number of training examples.
  sample_buffer_size: 0

eval:
  eval_delay: 18000  # Every 5 hours.

infer:
  batch_size: 3
